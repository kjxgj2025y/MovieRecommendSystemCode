{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、获取电影首页页面海报\n",
    "```\n",
    "字段如下：\n",
    "movie_id：影片ID\n",
    "movie_url：影片连接\n",
    "movie_home_poster：首页影片海报\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用fake_useragent 库中的 UserAgent 类来生成一个虚假的用户代理，以模拟浏览器的行为\n",
    "from fake_useragent import UserAgent\n",
    "# 用了 urllib.parse 模块中的 quote 函数来对URL进行编码\n",
    "# lxml 模块用于解析HTML响应\n",
    "import requests\n",
    "import time\n",
    "# 请求数据函数\n",
    "def request_data(url, headers, params):\n",
    "    res = None\n",
    "    num = 0\n",
    "    # 设置请求失败次数，不超过5次\n",
    "    while num <= 5:\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        response.encoding = \"utf-8\"\n",
    "        if response.status_code == 200:\n",
    "            res = response.json()\n",
    "            break\n",
    "        else:\n",
    "            num += 1\n",
    "            continue\n",
    "    return res\n",
    "\n",
    "\n",
    "# 保存数据\n",
    "data_save = open(file=\"../data/home_page_data\",mode=\"a\",encoding=\"utf-8\")\n",
    "# 定义列名\n",
    "name_list = 'movie_id$movie_url$movie_home_poster'+'\\n'\n",
    "data_save.write(name_list)\n",
    "def get_data():\n",
    "    ua = UserAgent().random\n",
    "    headers = {\n",
    "        'User-Agent': ua,\n",
    "        'Referer': 'https://movie.douban.com/explore'\n",
    "    }\n",
    "    url_base = 'https://m.douban.com/rexxar/api/v2/movie/recommend?'\n",
    "    for year in ['2019','2020','2021','2022','2023']:\n",
    "        params = {\n",
    "            'refresh': '0',\n",
    "            'start': '0',\n",
    "            'count': '350',\n",
    "            'selected_categories': '{}',\n",
    "            'uncollect': 'false',\n",
    "            'tags': year\n",
    "        }\n",
    "        data = request_data(url_base, headers, params)\n",
    "        for index in data['items']:\n",
    "            # 电影id\n",
    "            movie_id = index['id']\n",
    "            # 电影url\n",
    "            movie_url = 'https://movie.douban.com/subject/'+ movie_id + '/'\n",
    "            # 海报url\n",
    "            poster_url = requests.get(url=index['pic']['large']).content\n",
    "            # 电影海报保存路径\n",
    "            file_path =  '../data/image/' + movie_id + '/'\n",
    "            # 以这路径创建文件夹\n",
    "            if not os.path.exists(file_path):\n",
    "                os.makedirs(file_path)\n",
    "            # 电影海报图名称名称\n",
    "            movie_home_poster = file_path + 'p' + movie_id\n",
    "            with open(movie_home_poster + '.jpg', mode='wb') as f2:\n",
    "                f2.write(poster_url)\n",
    "            res = movie_id + '$' +movie_url + '$' + movie_home_poster + '\\n'\n",
    "            data_save.write(res)\n",
    "            print(res)\n",
    "            # 间隔1秒请求1次\n",
    "            time.sleep(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、获取电影详细页面数据\n",
    "```\n",
    "字段如下：\n",
    "movie_id：影片ID\n",
    "movie_name：影片名称\n",
    "movie_director：影片导演\n",
    "movie_writer：影片编剧\n",
    "movie_starring：影片主演\n",
    "movie_type：影片类型\n",
    "movie_country：影片制片国家\n",
    "movie_language：影片语言\n",
    "movie_release_date：影片上映日期\n",
    "movie_release_place 影片上映地方\n",
    "movie_run_time：影片片长\n",
    "movie_rating：影片评分\n",
    "rating_people_num：评分人数\n",
    "film_synopsis：影片剧情简介\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解释html和定位元素\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# 保存数据\n",
    "data_save = open(file=\"../data/detil_page_data.csv\",mode=\"a+\",encoding=\"utf-8\")\n",
    "# 定义列名\n",
    "name_list = 'movie_id$movie_name$movie_director$movie_writer$movie_starring$movie_type$\\\n",
    "movie_country$movie_language$movie_release_date$movie_release_place$movie_run_time$movie_rating$\\\n",
    "rating_people_num$film_synopsis' + '\\n'\n",
    "data_save.write(name_list)\n",
    "\n",
    "# 批量读取url\n",
    "file_path = pd.read_csv(\"../data/home_page_data\",sep=\"$\")\n",
    "for url in file_path['movie_url']:\n",
    "    ua = UserAgent().random\n",
    "    headers = {\n",
    "        'User-Agent': ua,\n",
    "        'Cookie': 'll=\"118281\"; bid=EIN7y1TWjCk; ct=y; push_noty_num=0; push_doumail_num=0; douban-fav-remind=1; ap_v=0,6.0'\n",
    "    }\n",
    "    response = requests.get(url, headers)\n",
    "    response.encoding = \"utf-8\"\n",
    "    if response.status_code == 200:\n",
    "        # 打印整个HTML文档的树状结构\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # 提取影片ID\n",
    "        movie_id = soup.find(\"div\",id=\"mainpic\").find(\"a\",class_=\"nbgnbg\")[\"href\"].split(\"subject\")[1].split(\"photos\")[0].strip(\"/\")\n",
    "        # 提取影片名称\n",
    "        movie_name = soup.find(\"div\",id=\"content\").find(\"h1\").find(\"span\").text\n",
    "        # 提取影片导演\n",
    "        movie_director = soup.find(id='info').find('span').find(\"span\",class_=\"attrs\").find(\"a\").text.strip()\n",
    "        # 提取影片编剧\n",
    "        movie_writer = soup.find(id='info').find_all('span')[3].text.replace(\"\\n\",\"\").replace(\" \",\"\").strip()\n",
    "        # 提取影片主演\n",
    "        movie_starring = soup.find(id='info').find_all('span')[6].text.replace(\"\\n\",\"\").replace(\" \",\"\")[:-5]+'...'\n",
    "        # 提取影片类型\n",
    "        movie_type = \"/\".join([i.text for i in soup.find(id='info').find_all('span', property='v:genre')])\n",
    "        # 提取影片制片国家\n",
    "        movie_country = soup.find(id='info').find(text='制片国家/地区:').next_element.strip().replace(\"\\n\",\"\").replace(\" \",\"\")\n",
    "        # 提取影片语言\n",
    "        movie_language = soup.find(id='info').find(text='语言:').next_element.strip().replace(\"\\n\",\"\").replace(\" \",\"\")\n",
    "        # 提取影片上映数据\n",
    "        release_data = soup.find(id='info').find_all('span', property='v:initialReleaseDate')[0].text\n",
    "        # 上映日期\n",
    "        movie_release_date = release_data.strip(\")\").split(\"(\")[0]\n",
    "        # 上映地方\n",
    "        movie_release_place = release_data.strip(\")\").split(\"(\")[1]\n",
    "        # 影片片长\n",
    "        movie_run_time = soup.find(id='info').find_all('span', property='v:runtime')[0].text\n",
    "        # 影片评分\n",
    "        movie_rating = soup.find(\"div\",class_=\"rating_wrap clearbox\").find(\"div\",class_=\"rating_self clearfix\")\\\n",
    "            .find(\"strong\").text.replace(\"\\n\",\"\").strip()\n",
    "        # 评分人数\n",
    "        rating_people_num = soup.find(\"div\",class_=\"rating_wrap clearbox\").find(\"div\",class_=\"rating_self clearfix\")\\\n",
    "            .find(\"div\",class_=\"rating_right\").find(\"div\",class_=\"rating_sum\").find(\"span\").text.replace(\"\\n\",\"\").strip()\n",
    "\n",
    "        # 提取电影剧情简介\n",
    "        film_synopsis = soup.find(\"div\",class_=\"related-info\").find(\"div\",class_=\"indent\")\\\n",
    "            .find(\"span\").text.replace(\"\\n\",\"\").replace(\" \",\"\").strip()\n",
    "        # 合并结果\n",
    "        res = movie_id + '$' + movie_name + '$' + movie_director + '$' + movie_writer + '$' + movie_starring + '$' + \\\n",
    "            movie_type + '$' + movie_country + '$' + movie_language + '$' + movie_release_date + '$' + movie_release_place + '$' + \\\n",
    "            movie_run_time + '$' + movie_rating + '$' + rating_people_num + '$' + film_synopsis + '\\n'\n",
    "        # 将结果输出\n",
    "        data_save.write(res)\n",
    "        print(res)\n",
    "    else:\n",
    "        print(\"请求失败，状态码：\", response.status_code)\n",
    "        # 请求数据失败时数值赋值为\"None\"\n",
    "        res = 'None$None$None$None$None$None$None$None$None$None$None$None$None$None$None$None$None' + '\\n'\n",
    "        # 将结果输出\n",
    "        data_save.write(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3、电影评论数据\n",
    "```\n",
    "字段如下：\n",
    "movie_id：影片ID\n",
    "user_id：用户ID\n",
    "user_rating：用户评分\n",
    "timestamp：评论时间\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  \n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "\n",
    "# 保存数据\n",
    "data_save = open(file=\"../data/comment_page_data.csv\",mode=\"a+\",encoding=\"utf-8\")\n",
    "# 定义列名\n",
    "name_list = 'movie_id$user_id$user_rating$time_stamp'+'\\n'\n",
    "data_save.write(name_list)\n",
    "\n",
    "# 电影评论url\n",
    "file_path = pd.read_csv(\"../data/home_page_data\",sep=\"$\")\n",
    "for url in file_path['movie_url']:\n",
    "    ua = UserAgent().random\n",
    "    headers = {\n",
    "        'User-Agent': ua,\n",
    "        'Cookie': ''\n",
    "    }\n",
    "    for score in ['h','m','l']:\n",
    "        params = {\n",
    "            'percent_type': score,\n",
    "            'limit': '600',\n",
    "            'status': 'P',\n",
    "            'sort': 'new_score',\n",
    "        }\n",
    "        response = requests.get(url=url, params=params, headers=headers)\n",
    "        response.encoding = \"utf-8\"\n",
    "        if response.status_code == 200:\n",
    "            # 使用lxml解析响应内容  \n",
    "            tree = etree.HTML(response.content)  \n",
    "            # movie_id：影片ID\n",
    "            movie_id = [mid.replace(\"\\n\",\"\").strip().split(\"subject\")[1].split(\"comments?\")[0]\\\n",
    "            .strip(\"/\") for mid in tree.xpath('//*[@id=\"comments\"]/div/div[2]/div/@data-url')]\n",
    "            # user_id：用户ID\n",
    "            user_id = [uid.replace(\"\\n\",\"\").strip().split(\"people\")[1].strip(\"/\") for uid in tree.xpath('//*[@id=\"comments\"]/div/div[1]/a/@href')]\n",
    "            # user_rating：评分\n",
    "            comm_rating = [comm_rat.replace(\"\\n\",\"\").strip() for comm_rat in tree.xpath('//*[@id=\"comments\"]/div/div[2]/h3/span[2]/span[2]/@class')]\n",
    "            # 评论时间戳(将标准时间格式转换为时间戳)\n",
    "            time_stamp = [str(int(time.mktime(time.strptime(time_str.strip(), \"%Y-%m-%d %H:%M:%S\")))) for time_str in tree.xpath('//*[@id=\"comments\"]/div/div[2]/h3/span[2]/span[3]/@title')]\n",
    "            # 取这4个字段长度最小作为索引值(目的解决list index out of range)\n",
    "            list_num = [len(movie_id),len(user_id),len(comm_rating),len(time_stamp)]\n",
    "            list_num.sort()\n",
    "            for index_num in range(list_num[0]):\n",
    "                res = movie_id[index_num] + '$' + user_id[index_num] + '$' + comm_rating[index_num] + '$' + time_stamp[index_num] + '\\n'\n",
    "                data_save.write(res)\n",
    "                print(res)\n",
    "        else:\n",
    "            print(\"请求失败，状态码：\", response.status_code)\n",
    "            res = 'None$None$None$None' + '\\n'\n",
    "            data_save.write(res)\n",
    "            print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4、用户标签数据集\n",
    "```\n",
    "字段如下：\n",
    "movie_id：影片ID\n",
    "user_id：用户ID\n",
    "user_tag：用户标签\n",
    "timestamp：评论时间\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 将电影数据和用户评论数据，以movie_id为主键合并数据集\n",
    "data1 = pd.read_csv(\"../data/detil_data.csv\",sep=\"$\")\n",
    "data2 = pd.read_csv(\"../data/comment_data.csv\",sep=\"$\")\n",
    "sub_data = data1.merge(data2,on=\"movie_id\",how=\"inner\")\n",
    "df = sub_data[['movie_id','user_id','movie_type','time_stamp']].drop_duplicates()\n",
    "# 将 movie_type 列中的每个类型以 '/' 分割，并将结果保存到新的行中  \n",
    "new_rows = []\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        types = row['movie_type'].split('/')\n",
    "        for type in types:\n",
    "            new_rows.append([row['movie_id'], row['user_id'], type, row['time_stamp']])  \n",
    "    except Exception as e:\n",
    "        for type in types:\n",
    "            new_rows.append([row['movie_id'], row['user_id'], \"None\", row['time_stamp']])  \n",
    "# 创建新的 DataFrame，并将新的行添加到其中，并movie_type列改为user_tag\n",
    "new_df = pd.DataFrame(new_rows, columns=df.columns).rename(columns={\"movie_type\":\"user_tag\"})\n",
    "new_df.to_csv(\"../data/tag_data.csv\",sep=\"$\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5284475075374502"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall = 5.6329497534853905\n",
    "precision = 0.8841807909604562\n",
    "F1 = (2*precision*recall)/(precision+recall)\n",
    "F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['喜剧', '悬疑', '犯罪', '惊悚', '恐怖', '剧情', '科幻', '动画', '冒险', '历史', '动作', '奇幻', '武侠', '古装', '爱情', '纪录片', '运动', '同性', '战争', '音乐', '灾难', '传记', '歌舞', '家庭', '短片', '儿童', '真人秀', '戏曲', '情色', '脱口秀', '西部']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"../data/wp/Tag-1.csv\",sep=\",\",index_col=False)\n",
    "print(data['user_tag'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['喜剧', '悬疑', '犯罪', '惊悚', '恐怖', '剧情', '科幻', '动画', '冒险', '历史', '动作', '奇幻', '武侠', '古装', '爱情', '纪录片', '运动', '同性', '战争', '音乐', '灾难', '传记', '歌舞', '家庭', '短片', '儿童', '真人秀', '戏曲', '情色', '脱口秀', '西部']\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
